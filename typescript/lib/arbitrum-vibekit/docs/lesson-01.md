# **Lesson 1: What is an AI Agent in Our Framework?**

---

### üîç Overview

An AI agent in our framework is a lightweight, self-contained service that exposes one or more tools (functions) to perform useful work‚Äîtypically with the help of a language model (LLM). It can be called directly (via HTTP or MCP) or indirectly (via another agent using A2A). Some agents are purely stateless. Others manage memory, long-running tasks, or even payment logic.

Agents can run anywhere: on your laptop, in the cloud, or inside a container. The framework is designed to be simple, pluggable, and understandable by junior developers with minimal setup.

---

### üõ† What Does an Agent Do?

- **Expose a single entrypoint tool**, e.g. `askXAgent`, which accepts free-form natural-language requests. Internally, the agent orchestrates other tools or agents under the hood.
- **Accept input** from a user, another agent, or a language model.
- **Run those tools** with optional logic before/after each call.
- **Return a response**, which might be plain data, a signed transaction, a streamed answer, or a delegated task.

---

### üåê Agent Communication

> ‚ÑπÔ∏è **Note:** In our framework, every agent acts as both an MCP server **and** an MCP client. This means agents can both expose themselves as tools (for LLMs or other agents to call) and also call other agents as tools. Under the hood, all tool calls follow the same A2A schema‚Äîso agents can be chained, composed, or orchestrated like Lego blocks using a shared protocol.

Agents can be called in multiple ways:

- **HTTP or CLI**
- **Model Context Protocol (MCP)**

  - Used when an LLM is calling your agent as a tool (not direct A2A)
  - Schema-based and compatible with many LLM frameworks

- **Agent-to-Agent Protocol (A2A)**

  - Used when another agent wants to delegate work to yours
  - Supports long-running tasks and streams

### ‚öñ Stateless vs. Stateful Agents

- **Stateless agents**:

  - Don't remember anything between calls
  - Easy to cache and scale

- **Stateful agents**:

  - Can track long-running tasks, store memory, or manage workflows
  - Use simple built-in helpers to manage global or per-task state

You choose which model you need per use case.

---

### üí° Why Agents Are Useful

Agents provide a clean boundary around logic you want to:

- Isolate
- Secure
- Reuse
- Chain together with other agents

They simplify LLM-driven workflows, let you enforce logic and validation, and allow you to charge for services via micro-payments if desired.

---

### ‚úÖ Summary

An **agent is a tool-powered, LLM-friendly function server** that can be stateless or stateful, and can collaborate with other agents over a standard protocol. It handles requests, executes logic, manages optional state, and returns structured results‚Äîall in a way that's easy to scale and extend.

> ‚ÄúAgents are just functions with context, coordination, and control‚Äîwrapped in a simple service shell.‚Äù

| Decision                            | Rationale                                                                                                                   |
| ----------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **Single-entry tool (`askXAgent`)** | Keeps the public interface tiny and self-describing for LLMs; avoids ‚Äúmany-tool sprawl,‚Äù which bloats prompt context.       |
| **Single-responsibility rule**      | Each agent owns one domain (price feed, wallet ops, etc.). Makes reasoning, scaling, and charging simpler.                  |
| **Dual role (MCP server + client)** | Lets any agent call or be called without extra glue; aligns with ‚Äúlego block‚Äù vision.                                       |
| **A2A as internal language**        | Guarantees every agent understands every other‚Äîno matter which outer wrapper (MCP, CLI, webhook) invoked it.                |
| **Swarm narrative**                 | Explicitly encourages composing many focused agents instead of one mega-agent, preventing context bloat and latency spikes. |
